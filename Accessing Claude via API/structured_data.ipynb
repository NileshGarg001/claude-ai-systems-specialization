{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93b5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Create an API Client \n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0506a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions \n",
    "def add_user_message(messages,text):\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "\n",
    "def add_assistant_message(messages,text):\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "\n",
    "def chat(messages, system = \"You are a helpful assistant.\", stop_sequences = None):\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2000,\n",
    "        messages = messages,\n",
    "        system=system,\n",
    "        stop_sequences=stop_sequences\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f076fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"title\": \"The Night Circus\",\n",
      "  \"author\": \"Erin Morgenstern\",\n",
      "  \"publishedYear\": 2011,\n",
      "  \"genres\": [\"Fantasy\", \"Romance\", \"Historical Fiction\", \"Magic Realism\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Trying to generate JSON with normal claude API \n",
    "messages = []\n",
    "add_user_message(messages, \"Generate a JSON object for a single book. Include the title, author's full name, published year, and a list of genres.\")\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270785f",
   "metadata": {},
   "source": [
    "So, We the problem here right.   \n",
    "If this was a response in block of cell that you want to copy all, you don't want the '''json in front \n",
    "and ''' in back.   \n",
    "To fix this, We use structured formatting with the assistance of prefilled assistant message and stop sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f00955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"title\": \"The Hobbit\",\n",
      "  \"author\": \"John Ronald Reuel Tolkien\",\n",
      "  \"publishedYear\": 1937,\n",
      "  \"genres\": [\n",
      "    \"Fantasy\",\n",
      "    \"Adventure\",\n",
      "    \"Children's Literature\",\n",
      "    \"High Fantasy\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Generate a JSON object for a single book. Include the title, author's full name, published year, and a list of genres.\")\n",
    "add_assistant_message(messages, \"```json\")\n",
    "response = chat(messages, stop_sequences=[\"```\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f051715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Hobbit',\n",
       " 'author': 'John Ronald Reuel Tolkien',\n",
       " 'publishedYear': 1937,\n",
       " 'genres': ['Fantasy', 'Adventure', \"Children's Literature\", 'High Fantasy']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "json.loads(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec84bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function for scraping a simple website using the `requests` and `BeautifulSoup` libraries:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "from urllib.parse import urljoin, urlparse\n",
      "\n",
      "def scrape_website(url, headers=None, timeout=10):\n",
      "    \"\"\"\n",
      "    Scrape a simple website and extract basic information.\n",
      "    \n",
      "    Args:\n",
      "        url (str): The URL to scrape\n",
      "        headers (dict): Optional headers to include in the request\n",
      "        timeout (int): Request timeout in seconds\n",
      "    \n",
      "    Returns:\n",
      "        dict: Dictionary containing scraped data\n",
      "    \"\"\"\n",
      "    \n",
      "    # Default headers to mimic a real browser\n",
      "    default_headers = {\n",
      "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
      "    }\n",
      "    \n",
      "    if headers:\n",
      "        default_headers.update(headers)\n",
      "    \n",
      "    try:\n",
      "        # Make the request\n",
      "        response = requests.get(url, headers=default_headers, timeout=timeout)\n",
      "        response.raise_for_status()  # Raise an exception for bad status codes\n",
      "        \n",
      "        # Parse the HTML content\n",
      "        soup = BeautifulSoup(response.content, 'html.parser')\n",
      "        \n",
      "        # Extract basic information\n",
      "        scraped_data = {\n",
      "            'url': url,\n",
      "            'status_code': response.status_code,\n",
      "            'title': soup.title.string.strip() if soup.title else 'No title found',\n",
      "            'meta_description': '',\n",
      "            'headings': {\n",
      "                'h1': [],\n",
      "                'h2': [],\n",
      "                'h3': []\n",
      "            },\n",
      "            'links': [],\n",
      "            'images': [],\n",
      "            'text_content': ''\n",
      "        }\n",
      "        \n",
      "        # Extract meta description\n",
      "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
      "        if meta_desc:\n",
      "            scraped_data['meta_description'] = meta_desc.get('content', '')\n",
      "        \n",
      "        # Extract headings\n",
      "        for i in range(1, 4):  # h1, h2, h3\n",
      "            headings = soup.find_all(f'h{i}')\n",
      "            scraped_data['headings'][f'h{i}'] = [h.get_text().strip() for h in headings]\n",
      "        \n",
      "        # Extract links\n",
      "        links = soup.find_all('a', href=True)\n",
      "        for link in links:\n",
      "            href = link['href']\n",
      "            text = link.get_text().strip()\n",
      "            absolute_url = urljoin(url, href)\n",
      "            scraped_data['links'].append({\n",
      "                'url': absolute_url,\n",
      "                'text': text\n",
      "            })\n",
      "        \n",
      "        # Extract images\n",
      "        images = soup.find_all('img', src=True)\n",
      "        for img in images:\n",
      "            src = img['src']\n",
      "            alt = img.get('alt', '')\n",
      "            absolute_url = urljoin(url, src)\n",
      "            scraped_data['images'].append({\n",
      "                'src': absolute_url,\n",
      "                'alt': alt\n",
      "            })\n",
      "        \n",
      "        # Extract text content (remove scripts and styles)\n",
      "        for script in soup([\"script\", \"style\"]):\n",
      "            script.decompose()\n",
      "        scraped_data['text_content'] = soup.get_text().strip()\n",
      "        \n",
      "        return scraped_data\n",
      "        \n",
      "    except requests.exceptions.RequestException as e:\n",
      "        return {\n",
      "            'error': f'Request error: {str(e)}',\n",
      "            'url': url\n",
      "        }\n",
      "    except Exception as e:\n",
      "        return {\n",
      "            'error': f'Parsing error: {str(e)}',\n",
      "            'url': url\n",
      "        }\n",
      "\n",
      "# Alternative simpler function for basic text extraction\n",
      "def simple_scrape(url):\n",
      "    \"\"\"\n",
      "    Simple function to just get the text content of a webpage.\n",
      "    \n",
      "    Args:\n",
      "        url (str): The URL to scrape\n",
      "    \n",
      "    Returns:\n",
      "        str: The text content of the webpage\n",
      "    \"\"\"\n",
      "    try:\n",
      "        headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
      "        }\n",
      "        \n",
      "        response = requests.get(url, headers=headers, timeout=10)\n",
      "        response.raise_for_status()\n",
      "        \n",
      "        soup = BeautifulSoup(response.content, 'html.parser')\n",
      "        \n",
      "        # Remove script and style elements\n",
      "        for script in soup([\"script\", \"style\"]):\n",
      "            script.decompose()\n",
      "        \n",
      "        return soup.get_text().strip()\n",
      "        \n",
      "    except Exception as e:\n",
      "        return f\"Error scraping {url}: {str(e)}\"\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Example 1: Detailed scraping\n",
      "    url = \"https://example.com\"\n",
      "    result = scrape_website(url)\n",
      "    \n",
      "    if 'error' not in result:\n",
      "        print(f\"Title: {result['title']}\")\n",
      "        print(f\"Meta Description: {result['meta_description']}\")\n",
      "        print(f\"Number of H1 tags: {len(result['headings']['h1'])}\")\n",
      "        print(f\"Number of links: {len(result['links'])}\")\n",
      "        print(f\"Number of images: {len(result['images'])}\")\n",
      "    else:\n",
      "        print(f\"Error: {result['error']}\")\n",
      "    \n",
      "    # Example 2: Simple text extraction\n",
      "    text_content = simple_scrape(url)\n",
      "    print(f\"Text content length: {len(text_content)} characters\")\n",
      "```\n",
      "\n",
      "## Installation Requirements\n",
      "\n",
      "Before using this code, you'll need to install the required packages:\n",
      "\n",
      "```bash\n",
      "pip install requests beautifulsoup4 lxml\n",
      "```\n",
      "\n",
      "## Key Features:\n",
      "\n",
      "1. **Error Handling**: Handles network errors and parsing errors gracefully\n",
      "2. **Headers**: Uses realistic browser headers to avoid being blocked\n",
      "3. **Comprehensive Data Extraction**: Gets title, meta description, headings, links, images, and text content\n",
      "4. **URL Resolution**: Converts relative URLs to absolute URLs\n",
      "5. **Content Cleaning**: Removes script and style tags for cleaner text extraction\n",
      "\n",
      "## Important Notes:\n",
      "\n",
      "1. **Respect robots.txt**: Always check the website's robots.txt file\n",
      "2. **Rate Limiting**: Add delays between requests for multiple pages\n",
      "3. **Legal Compliance**: Ensure you have permission to scrape the website\n",
      "4. **Terms of Service**: Check the website's terms of service\n",
      "5. **Dynamic Content**: This won't work for JavaScript-rendered content (use Selenium for that)\n",
      "\n",
      "## For JavaScript-heavy sites, you might need Selenium:\n",
      "\n",
      "```python\n",
      "# Alternative for JavaScript-rendered content\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.options import Options\n",
      "\n",
      "def scrape_dynamic_website(url):\n",
      "    chrome_options = Options()\n",
      "    chrome_options.add_argument(\"--headless\")\n",
      "    \n",
      "    driver = webdriver.Chrome(options=chrome_options)\n",
      "    driver.get(url)\n",
      "    \n",
      "    # Wait for content to load\n",
      "    time.sleep(3)\n",
      "    \n",
      "    html = driver.page_source\n",
      "    driver.quit()\n",
      "    \n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "    return soup.get_text().strip()\n",
      "```\n",
      "\n",
      "Remember to always be respectful when scraping websites and follow ethical scraping practices!\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Write a python function for scrapping a simple website\")\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14899111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "from urllib.parse import urljoin, urlparse\n",
      "\n",
      "def scrape_website(url, headers=None, timeout=10, parse_links=False):\n",
      "    \"\"\"\n",
      "    Simple website scraper function\n",
      "    \n",
      "    Args:\n",
      "        url (str): The URL to scrape\n",
      "        headers (dict): Optional headers to include in the request\n",
      "        timeout (int): Request timeout in seconds\n",
      "        parse_links (bool): Whether to extract all links from the page\n",
      "    \n",
      "    Returns:\n",
      "        dict: Dictionary containing scraped data\n",
      "    \"\"\"\n",
      "    \n",
      "    # Default headers to avoid being blocked\n",
      "    if headers is None:\n",
      "        headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
      "        }\n",
      "    \n",
      "    try:\n",
      "        # Make the request\n",
      "        response = requests.get(url, headers=headers, timeout=timeout)\n",
      "        response.raise_for_status()  # Raise an exception for bad status codes\n",
      "        \n",
      "        # Parse the HTML content\n",
      "        soup = BeautifulSoup(response.content, 'html.parser')\n",
      "        \n",
      "        # Extract basic information\n",
      "        result = {\n",
      "            'url': url,\n",
      "            'status_code': response.status_code,\n",
      "            'title': '',\n",
      "            'meta_description': '',\n",
      "            'headings': {\n",
      "                'h1': [],\n",
      "                'h2': [],\n",
      "                'h3': []\n",
      "            },\n",
      "            'paragraphs': [],\n",
      "            'images': [],\n",
      "            'links': [] if parse_links else None\n",
      "        }\n",
      "        \n",
      "        # Extract title\n",
      "        title_tag = soup.find('title')\n",
      "        if title_tag:\n",
      "            result['title'] = title_tag.get_text().strip()\n",
      "        \n",
      "        # Extract meta description\n",
      "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
      "        if meta_desc:\n",
      "            result['meta_description'] = meta_desc.get('content', '').strip()\n",
      "        \n",
      "        # Extract headings\n",
      "        for heading_level in ['h1', 'h2', 'h3']:\n",
      "            headings = soup.find_all(heading_level)\n",
      "            result['headings'][heading_level] = [h.get_text().strip() for h in headings]\n",
      "        \n",
      "        # Extract paragraphs\n",
      "        paragraphs = soup.find_all('p')\n",
      "        result['paragraphs'] = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]\n",
      "        \n",
      "        # Extract images\n",
      "        images = soup.find_all('img')\n",
      "        for img in images:\n",
      "            img_data = {\n",
      "                'src': urljoin(url, img.get('src', '')),\n",
      "                'alt': img.get('alt', ''),\n",
      "                'title': img.get('title', '')\n",
      "            }\n",
      "            result['images'].append(img_data)\n",
      "        \n",
      "        # Extract links if requested\n",
      "        if parse_links:\n",
      "            links = soup.find_all('a', href=True)\n",
      "            for link in links:\n",
      "                link_data = {\n",
      "                    'href': urljoin(url, link['href']),\n",
      "                    'text': link.get_text().strip(),\n",
      "                    'title': link.get('title', '')\n",
      "                }\n",
      "                result['links'].append(link_data)\n",
      "        \n",
      "        return result\n",
      "        \n",
      "    except requests.exceptions.RequestException as e:\n",
      "        return {\n",
      "            'url': url,\n",
      "            'error': f\"Request failed: {str(e)}\",\n",
      "            'status_code': None\n",
      "        }\n",
      "    except Exception as e:\n",
      "        return {\n",
      "            'url': url,\n",
      "            'error': f\"Parsing failed: {str(e)}\",\n",
      "            'status_code': response.status_code if 'response' in locals() else None\n",
      "        }\n",
      "\n",
      "def scrape_multiple_pages(urls, delay=1, **kwargs):\n",
      "    \"\"\"\n",
      "    Scrape multiple pages with delay between requests\n",
      "    \n",
      "    Args:\n",
      "        urls (list): List of URLs to scrape\n",
      "        delay (int): Delay between requests in seconds\n",
      "        **kwargs: Additional arguments to pass to scrape_website\n",
      "    \n",
      "    Returns:\n",
      "        list: List of scraped data for each URL\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    \n",
      "    for i, url in enumerate(urls):\n",
      "        print(f\"Scraping {i+1}/{len(urls)}: {url}\")\n",
      "        result = scrape_website(url, **kwargs)\n",
      "        results.append(result)\n",
      "        \n",
      "        # Add delay between requests (except for the last one)\n",
      "        if i < len(urls) - 1:\n",
      "            time.sleep(delay)\n",
      "    \n",
      "    return results\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Example 1: Scrape a single website\n",
      "    url = \"https://httpbin.org/html\"\n",
      "    result = scrape_website(url, parse_links=True)\n",
      "    \n",
      "    print(\"Title:\", result.get('title'))\n",
      "    print(\"Status Code:\", result.get('status_code'))\n",
      "    print(\"Number of paragraphs:\", len(result.get('paragraphs', [])))\n",
      "    print(\"Number of images:\", len(result.get('images', [])))\n",
      "    print(\"Number of links:\", len(result.get('links', [])) if result.get('links') else 0)\n",
      "    \n",
      "    # Example 2: Scrape multiple websites\n",
      "    urls = [\n",
      "        \"https://httpbin.org/html\",\n",
      "        \"https://httpbin.org/robots.txt\"\n",
      "    ]\n",
      "    \n",
      "    results = scrape_multiple_pages(urls, delay=2)\n",
      "    for result in results:\n",
      "        print(f\"\\nURL: {result['url']}\")\n",
      "        print(f\"Status: {result.get('status_code', 'Error')}\")\n",
      "        if 'error' in result:\n",
      "            print(f\"Error: {result['error']}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Write a python function for scrapping a simple website\")\n",
    "add_assistant_message(messages, \"```python\")\n",
    "response = chat(messages, stop_sequences=[\"```\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
